# Apache Airflow Dockerfiles


В этом репозитории содержатся два докерфайла для развертывания Apache Airflow:

* `Dockerfile_good` - докерфайл, содержащий хорошие практики развертывания Apache Airflow
* `Dockerfile_bad` - докерфайл, содержащий плохие практики развертывания Apache Airflow

Мы рассмотрим плохие и хорошие практики, используемые в этих **Dockerfile**, и объясним, почему они являются такими. Кроме того, мы рассмотрим две плохие практики по использованию контейнеризации.

## Плохой Dockerfile

`Dockerfile_bad` содержит следующие плохие практики развертывания Apache Airflow:

### Команда для создания образа
```bash
docker build -f Dockerfile_bad -t bad_airflow .
```

### Плохой Dockerfile для Apache Airflow выглядит следующим образом:
```bash
FROM python:3.8

RUN pip install apache-airflow

WORKDIR /app

COPY . /app

RUN airflow db init

CMD ["airflow", "webserver", "--port", "8080"]

VOLUME /app/dags
```


### В этом Dockerfile используются следующие плохие практики:


1. Использование базового образа `python:3.8` для создания образа Apache Airflow. Это может привести к тому, что в образе не будут содержаться все необходимые компоненты и настройки для работы Apache Airflow.
2. Отсутствие выполнения обновления пакетов и установки необходимых зависимостей. Это может привести к тому, что в образе не будут установлены все необходимые зависимости для работы Apache Airflow.
3. Отсутствие использования отдельного пользователя для запуска процессов Apache Airflow. Это может привести к потенциальным проблемам с безопасностью, которые могут возникнуть при запуске процессов от имени пользователя `root`.
4. Копирование всего содержимого текущей директории в директорию `/app` в образе. Это может привести к тому, что в образе будут содержаться ненужные файлы и данные, которые могут замедлить работу Apache Airflow и увеличить размер образа.
5. Отсутствие выполнения инициализации базы данных Apache Airflow. Это может привести к тому, что база данных Apache Airflow не будет правильно инициализирована и не будет готова к работе.
6. Отсутствие использования `EXPOSE` для указания порта, на котором будет доступен веб-интерфейс Apache Airflow. Это может привести к тому, что веб-интерфейс Apache Airflow не будет доступен извне контейнера.
7. Отсутствие использования `VOLUME` для размещения DAG-файлов в контейнере. Это может привести к тому, что DAG-файлы не будут доступны в контейнере и не будут использоваться при выполнении заданий Apache Airflow.

## Хороший Dockerfile

`Dockerfile_good` содержит следующие хорошие практики развертывания Apache Airflow:

### Команда для создания образа

```bash
docker build -f Dockerfile_good -t good_airflow .
```

### Хороший Dockerfile для Apache Airflow выглядит следующим образом:

```bash
FROM apache/airflow:2.9.1

WORKDIR /opt/airflow

USER root 

RUN apt update && \
    apt install -y procps default-jre && \
    apt clean

USER airflow 

COPY ./dags/* ./dags/
COPY ./spark_jobs/* ./spark_jobs/
COPY ./data/* ./data/

RUN pip install apache-airflow-providers-apache-spark plyvel && \
    airflow db init

EXPOSE 8080

VOLUME /app/dags

CMD ["airflow", "webserver", "--port", "8080"]
```

### В этом Dockerfile используются следующие хорошие практики:

1. Использование официального образа Apache Airflow (`apache/airflow:2.9.1`). Это гарантирует, что в образе будут содержаться все необходимые компоненты и настройки для работы Apache Airflow.
2. Выполнение обновления пакетов и установка необходимых зависимостей (`procps` и `default-jre`) с помощью `apt`. Это гарантирует, что в образе будут установлены все необходимые зависимости для работы Apache Airflow.
3. Использование отдельного пользователя `airflow` для запуска процессов Apache Airflow. Это позволяет избежать потенциальных проблем с безопасностью, которые могут возникнуть при запуске процессов от имени пользователя `root`.
4. Копирование DAG-файлов, файлов с заданиями для Apache Spark и данных в соответствующие директории в образе. Это позволяет разместить эти файлы и данные в контейнере и использовать их при выполнении заданий Apache Airflow.
5. Выполнение инициализации базы данных Apache Airflow с помощью `airflow db init`. Это гарантирует, что база данных Apache Airflow будет правильно инициализирована и готова к работе.
6. Использование `EXPOSE` для указания порта, на котором будет доступен веб-интерфейс Apache Airflow. Это позволяет разрешить доступ к веб-интерфейсу Apache Airflow извне контейнера.
7. Использование `VOLUME` для размещения DAG-файлов в контейнере. Это позволяет разместить DAG-файлы в контейнере и использовать их при выполнении заданий Apache Airflow.

Эти практики позволяют создать надежный, безопасный и гибко настраиваемый образ контейнера для запуска Apache Airflow.


## Плохие практики по использованию контейнеризации

1. Использование одного контейнера для многих приложений или сервисов. Это может привести к проблемам с безопасностью, масштабируемостью и отказоустойчивостью.

2. Использование контейнеров для размещения приложений, которые требуют очень высокой производительности или очень низкой задержки. Контейнеры могут добавить дополнительную нагрузку на систему и увеличить время отклика приложений. В этом случае лучше использовать физические или виртуальные машины.

## Заключение

В этом репозитории мы рассмотрели плохой и хороший Dockerfile для Apache Airflow и объяснили, почему используемые в них практики являются такими. Кроме того, мы рассмотрели две плохие практики по использованию контейнеризации. Надеемся, что этот репозиторий поможет вам создавать более эффективные и безопасные контейнеры для Apache Airflow.